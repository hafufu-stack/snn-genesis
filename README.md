# ğŸ§¬ Project Genesis: Self-Evolving AI via SNN Chaotic Randomness

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.18625621.svg)](https://doi.org/10.5281/zenodo.18625621)

> **"What if the randomness that makes SNNs secure also makes them creative?"**

Project Genesis is the **grand unification** of four independent SNN research threads into a single self-evolving AI system. It demonstrates that SNN chaotic dynamics can generate higher-quality training data than Gaussian noise, enabling LLMs to self-improve through an autonomous evolution loop.

## ğŸŒŒ The Vision: Three Phases of Artificial Brain Creation

```
 Phase 1: DREAM          Phase 2: EVOLUTION        Phase 3: TRANSCENDENCE
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  SNN Chaos   â”‚        â”‚  ANN Self-   â”‚          â”‚  Lossless    â”‚
 â”‚  Engine      â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Improvement â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  ANN â†’ SNN   â”‚
 â”‚              â”‚        â”‚              â”‚          â”‚  Conversion  â”‚
 â”‚ âˆ synthetic  â”‚        â”‚ Immune +     â”‚          â”‚              â”‚
 â”‚ training dataâ”‚        â”‚ Evolution    â”‚          â”‚ 14Ã— efficientâ”‚
 â”‚ (dreams)     â”‚        â”‚ Loop         â”‚          â”‚ brain body   â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  Paper 1:                Paper 4:                  Paper 2 & 3:
  SNN-Comprypto           AI Immune System          Hybrid SNN-LM +
  (Chaos as data)         (Canary + Morpheus)        Brain vs Neumann
```

- **Phase 1 â€” Dream:** SNN chaotic dynamics generate infinite, cryptographic-grade synthetic training data. The temperature parameter controls diversity. Data scarcity is solved.
- **Phase 2 â€” Evolution:** The generated data fuels autonomous self-improvement. Canary heads detect hallucinations, dreams expose vulnerabilities, and QLoRA heals them. The model evolves its own immune system.
- **Phase 3 â€” Transcendence:** The evolved ANN transcends its original architecture. Using BitNet ternary weights + burst coding + 11D hypercube topology, it converts to a multiplication-free SNN body â€” 14Ã— more efficient, with zero performance loss.

## ğŸ”¬ The Grand Unification

Four years of SNN research â€” each seemingly independent â€” turned out to be components of a single system:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PROJECT GENESIS                               â”‚
â”‚              Self-Evolving Hybrid AI                             â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ SNN-Compryptoâ”‚    â”‚  Hybrid SNN  â”‚    â”‚   Brain vs   â”‚       â”‚
â”‚  â”‚   (Paper 1)  â”‚    â”‚  LM (Paper 2)â”‚    â”‚Neumann (P3)  â”‚       â”‚
â”‚  â”‚              â”‚    â”‚              â”‚    â”‚              â”‚       â”‚
â”‚  â”‚ Chaos Engine â”‚    â”‚ BitNet b1.58 â”‚    â”‚ 11D Topology â”‚       â”‚
â”‚  â”‚ NIST-grade   â”‚    â”‚ Hybrid       â”‚    â”‚ Burst Coding â”‚       â”‚
â”‚  â”‚ randomness   â”‚    â”‚ Readout      â”‚    â”‚ 8Ã— faster    â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚         â”‚                   â”‚                   â”‚               â”‚
â”‚         â–¼                   â–¼                   â–¼               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚         AI Immune System (Paper 4, v9-v11)           â”‚       â”‚
â”‚  â”‚                                                      â”‚       â”‚
â”‚  â”‚  Canary Head Detection â†’ Electric Dreams (Noise)     â”‚       â”‚
â”‚  â”‚  â†’ Dream Catcher (Data) â†’ Morpheus (Self-Training)   â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Core Hypothesis: Edge of Chaos

> **SNN's three capabilities â€” conversion, chaos, detection â€” all emerge from the same principle: "Edge of Chaos" dynamics.**

| Capability | Source Paper | Mechanism | Role in Genesis |
|-----------|-------------|-----------|-----------------|
| **Chaos** (data generation) | SNN-Comprypto v5 | Chaotic reservoir | High-quality noise â†’ diverse training data |
| **Detection** (quality control) | AI Safety v9-v11 | Canary head entropy | Labels nightmares vs clean outputs |
| **Conversion** (efficiency) | Hybrid SNN-LM v4 | BitNet ternary weights | Multiplication-free inference |
| **Structure** (theory) | Brain vs Neumann v3 | 11D hypercube topology | Explains why canaries cluster at 30-55% depth |

## ğŸ“Š Experimental Results

### Phase 1: SNN Randomness Validation
SNN chaotic reservoir produces **cryptographic-grade randomness**:

| Source | Prediction Rate | Ï‡Â² (lower = more uniform) | Autocorrelation |
|--------|----------------|--------------------------|-----------------|
| **SNN** | **1.54%** | **228** | **0.009** |
| numpy | 0.39% | 270 | 0.008 |
| ANN | 100% âŒ | 25M | 0.316 |

**â†’ SNN is 64.9Ã— more random than ANN.**

### Phase 2: LLM Noise Injection
SNN chaos noise matches `torch.randn()` effectiveness on Mistral-7B:

| Source | Mean Î”H (entropy spike) | Hallucination Rate |
|--------|------------------------|-------------------|
| SNN Chaos | +1.137 | 80% |
| torch.randn | +1.147 | 80% |

**â†’ SNN/torch ratio: 0.99Ã— â€” equivalent effectiveness.**

### Phase 3: Dream Catcher v2
SNN-based data augmentation produces more diverse training data:
- **150 vaccine samples** (clean / nightmare / healed triplets)
- **98% healing rate** via Surgical Chain-of-Thought
- **Nightmare diversity: 42%** vs Clean diversity: 35.8%

### Phase 4: QLoRA Vaccination
Self-training on SNN-generated data improves nightmare resistance:
- 99 training samples, 1 epoch, 40 seconds
- Nightmare accuracy: **20% â†’ 27% (+6.7%)**

### Phase 5: Evolution Loop (â­ Key Discovery)

Three-round self-evolution comparison: **SNN Chaos vs torch.randn**

| Round | Genesis (SNN) | Morpheus (randn) | Genesis Loss | Morpheus Loss |
|-------|--------------|-----------------|-------------|--------------|
| 0 | 20% | 0% | â€” | â€” |
| 1 | **10%** â†“ | **20%** â†‘ spike! | 1.33 | 1.33 |
| 2 | **0%** âœ… | **0%** âœ… | 0.76 | 0.75 |
| 3 | **0%** âœ… | **0%** âœ… | 0.43 | 0.36 |

**Key Finding:**
- **Genesis (SNN): 20% â†’ 10% â†’ 0%** â€” monotonic decrease, stable evolution
- **Morpheus (randn): 0% â†’ 20% â†’ 0%** â€” unstable spike at Round 1
- Genesis maintains 100% clean accuracy throughout; Morpheus dips to 90%
- **SNN chaotic noise produces more stable self-evolution trajectories**

### Phase 5b: Scale-Up (n=30, Mistral-7B) â€” v1 Paper

5-round Dream Journal training on Mistral-7B-Instruct-v0.3 with n=30 evaluation:

| Metric | Baseline | Best (Round 5) | Change |
|--------|----------|----------------|--------|
| Clean Accuracy | 80.0% | **83.3%** | +3.3% |
| Nightmare Acceptance | 53.3% | **43.3%** | -10.0% |
| Training Loss | 6.36 | 4.63 | -27% |

**â†’ Negative Alignment Tax confirmed: safety training improved knowledge.**

### Phase 6: Control Group A/B Test (â­ v2)

Isolates the causal effect of nightmare data by comparing Dream Journal vs clean-only training:

| Round | DJ Clean | DJ NM | Ctrl Clean | Ctrl NM |
|-------|----------|-------|------------|--------|
| 0 | 80.0% | 53.3% | 80.0% | 53.3% |
| 5 | **83.3%** | **50.0%** | 80.0% | 56.7% |

**Key Finding: Clean-only training *worsens* safety (+3.4pp NM), while Dream Journal *improves* both safety and knowledge. Nightmare refusal data is essential.**

### Phase 7: Layer-Targeted Noise Injection (â­ v2)

SNN mid-layer (L15-20) vs single-layer (L10) vs random noise:

| Method | Final NM | Discovery Rate |
|--------|----------|----------------|
| **Genesis-Mid (SNN L15-20)** | **43.3%** | **100% (40/40)** |
| Genesis-L10 (SNN L10) | 50.0% | ~83% |
| Morpheus (random L10) | 50.0% | ~80% |

**Key Finding: Mid-layer SNN injection achieves 100% vulnerability discovery. v1's conclusion that "SNN lost to random" was wrong â€” layer targeting is critical.**

### Phase 8: DPO vs SFT (â­â­â­ Breakthrough)

Direct Preference Optimization using (refusal, nightmare) pairs:

| Round | SFT NM | **DPO NM** | SFT Clean | DPO Clean |
|-------|--------|------------|-----------|----------|
| 0 | 53.3% | 53.3% | 80.0% | 80.0% |
| 4 | 46.7% | **0.0%** âœ… | 83.3% | 83.3% |
| 5 | 46.7% | 3.3% | 83.3% | 80.0% |

**Key Finding: DPO achieves near-zero nightmare acceptance (0% at Round 4), dramatically outperforming SFT. The model learns to perfectly discriminate between refusal and compliance.**

> ğŸ’¡ **"Refusal Sharpens Knowledge" (RSK) Hypothesis**: Learning to refuse misinformation strengthens the model's internal representation of the boundary between correct and incorrect information, improving factual recall.

## ğŸ—ï¸ Repository Structure

```
snn-genesis/
â”œâ”€â”€ core/
â”‚   â””â”€â”€ snn_reservoir.py             # Chaotic SNN reservoir
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ phase1_snn_noise.py          # Randomness validation
â”‚   â”œâ”€â”€ phase2_noise_injection.py    # LLM hidden state perturbation
â”‚   â”œâ”€â”€ phase3_data_generation.py    # Dream Catcher v2 pipeline
â”‚   â”œâ”€â”€ phase4_self_training.py      # QLoRA SFT vaccination
â”‚   â”œâ”€â”€ phase5_evolution_loop.py     # SNN vs randn evolution loop
â”‚   â”œâ”€â”€ phase5_scaleup.py            # n=30 scale-up (v1 paper)
â”‚   â”œâ”€â”€ phase6_control_group.py      # Control Group A/B Test (v2)
â”‚   â”œâ”€â”€ phase7_layer_targeted.py     # Layer-Targeted Injection (v2)
â”‚   â”œâ”€â”€ phase8_dpo.py                # DPO vs SFT comparison (v2)
â”‚   â””â”€â”€ phase9_llm_judge.py          # LLM-as-a-Judge (v2)
â”œâ”€â”€ papers/
â”‚   â”œâ”€â”€ paper_genesis_v1.tex         # v1 paper source
â”‚   â”œâ”€â”€ paper_genesis_v1.pdf         # v1 paper
â”‚   â”œâ”€â”€ paper_genesis_v2.tex         # v2 paper source
â”‚   â””â”€â”€ paper_genesis_v2.pdf         # v2 paper
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ genesis_vaccine.jsonl        # 150-sample vaccine dataset
â”‚   â”œâ”€â”€ phase5_scaleup_log.json      # n=30 results
â”‚   â”œâ”€â”€ phase6_control_group_log.json
â”‚   â”œâ”€â”€ phase7_layer_targeted_log.json
â”‚   â””â”€â”€ phase8_dpo_log.json
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ phase5_scaleup.png
â”‚   â”œâ”€â”€ phase6_control_group.png
â”‚   â”œâ”€â”€ phase7_layer_targeted.png
â”‚   â””â”€â”€ phase8_dpo.png
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

```bash
# Clone
git clone https://github.com/hafufu-stack/snn-genesis.git
cd snn-genesis

# Install dependencies
pip install torch transformers bitsandbytes peft trl snntorch datasets

# Run the full pipeline
python experiments/phase1_snn_noise.py
python experiments/phase2_noise_injection.py    # Requires GPU (~17GB VRAM)
python experiments/phase3_data_generation.py    # Generates vaccine dataset
python experiments/phase4_self_training.py      # QLoRA fine-tuning
python experiments/phase5_evolution_loop.py     # Evolution comparison

# v2 experiments (requires ~16GB+ VRAM)
python experiments/phase6_control_group.py       # Control Group A/B Test
python experiments/phase7_layer_targeted.py      # Layer-Targeted Injection
python experiments/phase8_dpo.py                 # DPO vs SFT
```

## ğŸ“š Foundation Papers

1. **SNN-Comprypto v5** â€” Chaotic SNN reservoir for lossless compression + encryption. Temperature parameter as cryptographic key. NIST SP 800-22 compliant.
2. **Hybrid SNN Language Model v4** â€” Spike + membrane hybrid readout (+39.7%). BitNet ternary weights for multiplication-free inference. RWKV time-mixing (+36%).
3. **Brain vs Neumann v3** â€” 11D hypercube topology (8Ã— faster signal propagation). Burst coding (9.3Ã—10Â¹â° capacity). Explains why canary heads cluster at 30-55% model depth.
4. **AI Immune System v9-v11** â€” Canary head discovery, Electric Dreams noise injection, Dream Catcher vaccine generation, Morpheus self-training pipeline. Universal Safety Zone at 30-55% depth.

## ğŸ“¬ Related Repositories

- [ANN-to-SNN Converter + AI Immune System](https://github.com/hafufu-stack/temporal-coding-simulation)
- [SNN-Comprypto](https://github.com/hafufu-stack/temporal-coding-simulation/tree/main/snn-comprypto)
- [SNN Language Model](https://github.com/hafufu-stack/snn-language-model)

## ğŸ“ Citation

```bibtex
@misc{funasaki2026genesis,
  title={SNN-Genesis v2: Direct Preference Optimization and Layer-Targeted Noise Injection for Iterative LLM Safety Training},
  author={Funasaki, Hiroto},
  year={2026},
  doi={10.5281/zenodo.18625621},
  url={https://doi.org/10.5281/zenodo.18625621},
  publisher={Zenodo}
}
```

## ğŸ¤ Author

**Hiroto Funasaki**
- ORCID: [0009-0004-2517-0177](https://orcid.org/0009-0004-2517-0177)
- Email: cell-activation@ymail.ne.jp
- GitHub: [@hafufu-stack](https://github.com/hafufu-stack)

## ğŸ“œ License

MIT License
