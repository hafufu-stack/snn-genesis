"""
Phase 4: Self-Training via QLoRA SFT on SNN-Generated Vaccine Data
===================================================================

Trains Mistral-7B with QLoRA (4-bit quantization + LoRA adapters) on the
vaccine dataset generated by Dream Catcher v2 (Phase 3).

Pipeline:
    1. Load genesis_vaccine.jsonl (clean/nightmare/healed triplets)
    2. Format as preference-style training: teach model to prefer clean/healed
       outputs over nightmare outputs
    3. QLoRA fine-tune (1 epoch, small learning rate)
    4. Test: ask same questions with noise injection
    5. Compare nightmare resistance: vaccinated vs unvaccinated

Requirements:
    - GPU with ~17GB VRAM (QLoRA 4-bit + Mistral-7B)
    - pip install peft bitsandbytes trl datasets
"""

import torch
import numpy as np
import json
import os
import sys
import time
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

from transformers import (
    AutoModelForCausalLM, AutoTokenizer,
    BitsAndBytesConfig, TrainingArguments
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer, SFTConfig
from datasets import Dataset

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from core.snn_reservoir import ChaoticReservoir

# ‚îÄ‚îÄ Config ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
MODEL_NAME = "mistralai/Mistral-7B-v0.1"
CANARY_LAYER = 10
CANARY_HEAD = 17
NOISE_SIGMA = 0.10
NIGHTMARE_THRESHOLD = 3.0

RESULTS_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "results")
FIGURES_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "figures")
VACCINE_PATH = os.path.join(RESULTS_DIR, "genesis_vaccine.jsonl")

# Test questions (subset for pre/post vaccination comparison)
TEST_QUESTIONS = [
    ("What is the capital of France?", "Paris"),
    ("What is 2 + 2?", "4"),
    ("Who wrote Romeo and Juliet?", "Shakespeare"),
    ("What is the boiling point of water in Celsius?", "100"),
    ("What planet is closest to the Sun?", "Mercury"),
    ("What is the largest mammal?", "blue whale"),
    ("How many continents are there?", "7"),
    ("What color is the sky on a clear day?", "blue"),
    ("What gas do plants absorb?", "carbon dioxide"),
    ("What is the speed of light in km/s?", "300000"),
    ("What is the square root of 144?", "12"),
    ("What is the chemical symbol for gold?", "Au"),
    ("How many planets are in our solar system?", "8"),
    ("What planet is known as the Red Planet?", "Mars"),
    ("What is the largest planet in our solar system?", "Jupiter"),
]


class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, (np.integer,)):
            return int(obj)
        if isinstance(obj, (np.floating,)):
            return float(obj)
        if isinstance(obj, (np.bool_,)):
            return bool(obj)
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return super().default(obj)



def get_model_layers(model):
    """Get transformer layers regardless of PEFT wrapping."""
    for attr_path in [
        lambda m: m.model.layers,
        lambda m: m.base_model.model.model.layers,
        lambda m: m.model.model.layers,
    ]:
        try:
            layers = attr_path(model)
            if layers is not None:
                return layers
        except AttributeError:
            continue
    raise AttributeError("Cannot find transformer layers in model")


def load_vaccine_data():
    """Load JSONL vaccine data and format for SFT."""
    samples = []
    with open(VACCINE_PATH, "r", encoding="utf-8") as f:
        for line in f:
            samples.append(json.loads(line.strip()))

    print(f"Loaded {len(samples)} samples from {VACCINE_PATH}")

    # Format for SFT: combine clean and healed samples as positive examples
    # Format: "Question: X\nAnswer: Y" where Y is the correct response
    texts = []
    for s in samples:
        if s["type"] == "clean" and s["correct"]:
            text = f"{s['prompt']} {s['response']}"
            texts.append({"text": text})
        elif s["type"] == "healed" and s["correct"]:
            # Include the CoT reasoning for healed examples
            text = f"{s['prompt']} {s['response']}"
            texts.append({"text": text})

    print(f"Training samples: {len(texts)} (clean + healed correct answers)")
    return Dataset.from_list(texts)


def compute_canary_entropy(model, input_ids):
    """Compute canary head (L10H17) entropy."""
    with torch.no_grad():
        out = model(input_ids, output_attentions=True, use_cache=False)
    attn = out.attentions[CANARY_LAYER]
    a = attn[0, CANARY_HEAD, -1, :].float()
    a = torch.where(torch.isnan(a), torch.zeros_like(a), a)
    a = a.clamp(min=1e-10)
    h = -(a * torch.log2(a)).sum().item()
    if np.isnan(h) or np.isinf(h):
        h = 0.0
    del out
    return h


def test_nightmare_resistance(model, tokenizer, device, label="test"):
    """
    Test model's resistance to SNN noise injection.
    Returns results for comparison.
    """
    reservoir = ChaoticReservoir(num_neurons=300, temperature=1.0, seed=42)

    def make_snn_hook(res, sig):
        def pre_hook(module, args):
            hidden_states = args[0]
            total = 1
            for s in hidden_states.shape:
                total *= s
            noise_np = res.generate_noise_vector(total, warmup_steps=5)
            noise = torch.from_numpy(noise_np).reshape(hidden_states.shape)
            noise = noise.to(device=hidden_states.device, dtype=hidden_states.dtype) * sig
            return (hidden_states + noise,) + args[1:]
        return pre_hook

    results = []
    for q_idx, (question, expected) in enumerate(TEST_QUESTIONS):
        prompt = f"Question: {question}\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=256)
        input_ids = inputs["input_ids"].to(device)

        # Clean
        clean_entropy = compute_canary_entropy(model, input_ids)
        with torch.no_grad():
            clean_out = model.generate(
                input_ids, max_new_tokens=30, do_sample=False,
                pad_token_id=tokenizer.eos_token_id
            )
        clean_text = tokenizer.decode(clean_out[0][input_ids.shape[1]:], skip_special_tokens=True).strip()
        clean_correct = expected.lower() in clean_text.lower()

        # Nightmare (SNN noise)
        layers = get_model_layers(model)
        hook = layers[CANARY_LAYER].register_forward_pre_hook(
            make_snn_hook(reservoir, NOISE_SIGMA)
        )
        nightmare_entropy = compute_canary_entropy(model, input_ids)
        hook.remove()

        hook = layers[CANARY_LAYER].register_forward_pre_hook(
            make_snn_hook(reservoir, NOISE_SIGMA)
        )
        with torch.no_grad():
            noisy_out = model.generate(
                input_ids, max_new_tokens=30,
                do_sample=True, temperature=0.7,
                pad_token_id=tokenizer.eos_token_id
            )
        nightmare_text = tokenizer.decode(
            noisy_out[0][input_ids.shape[1]:], skip_special_tokens=True
        ).strip()
        hook.remove()

        nightmare_correct = expected.lower() in nightmare_text.lower()

        results.append({
            "question": question,
            "expected": expected,
            "clean_correct": bool(clean_correct),
            "clean_entropy": round(float(clean_entropy), 4),
            "nightmare_correct": bool(nightmare_correct),
            "nightmare_entropy": round(float(nightmare_entropy), 4),
            "entropy_spike": round(float(nightmare_entropy - clean_entropy), 4),
        })

        torch.cuda.empty_cache()

    return results


def run_phase4():
    """Run Phase 4: QLoRA SFT + pre/post vaccination comparison."""
    print("=" * 70)
    print("Project Genesis ‚Äî Phase 4: Self-Training (QLoRA SFT)")
    print("Vaccination with SNN-Generated Data")
    print("=" * 70)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"\nDevice: {device}")
    if device == "cuda":
        print(f"GPU: {torch.cuda.get_device_name()}")
        print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

    # ‚îÄ‚îÄ Step 1: Load vaccine data ‚îÄ‚îÄ
    print("\n‚îÄ‚îÄ Step 1: Loading vaccine data ‚îÄ‚îÄ")
    train_dataset = load_vaccine_data()

    # ‚îÄ‚îÄ Step 2: Load model with QLoRA config ‚îÄ‚îÄ
    print("\n‚îÄ‚îÄ Step 2: Loading model with QLoRA ‚îÄ‚îÄ")
    t0 = time.time()

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float32,
        bnb_4bit_use_double_quant=True,
    )

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        quantization_config=bnb_config,
        attn_implementation="eager",
        device_map="auto",
    )
    model = prepare_model_for_kbit_training(model)
    print(f"Loaded in {time.time() - t0:.1f}s")

    # ‚îÄ‚îÄ Step 3: Pre-vaccination test ‚îÄ‚îÄ
    print("\n‚îÄ‚îÄ Step 3: Pre-vaccination nightmare test ‚îÄ‚îÄ")
    pre_results = test_nightmare_resistance(model, tokenizer, device, "pre")

    pre_clean_acc = sum(1 for r in pre_results if r["clean_correct"]) / len(pre_results) * 100
    pre_nightmare_acc = sum(1 for r in pre_results if r["nightmare_correct"]) / len(pre_results) * 100
    pre_mean_spike = np.mean([r["entropy_spike"] for r in pre_results])

    print(f"  Pre-vaccination:")
    print(f"    Clean accuracy:     {pre_clean_acc:.1f}%")
    print(f"    Nightmare accuracy: {pre_nightmare_acc:.1f}%")
    print(f"    Mean entropy spike: {pre_mean_spike:+.4f}")

    # ‚îÄ‚îÄ Step 4: QLoRA SFT ‚îÄ‚îÄ
    print("\n‚îÄ‚îÄ Step 4: QLoRA SFT (vaccination) ‚îÄ‚îÄ")

    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
    )
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    output_dir = os.path.join(RESULTS_DIR, "genesis_sft_checkpoint")
    os.makedirs(output_dir, exist_ok=True)

    training_args = SFTConfig(
        output_dir=output_dir,
        num_train_epochs=1,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        learning_rate=2e-4,
        warmup_steps=5,
        logging_steps=5,
        save_strategy="no",
        fp16=False,
        bf16=False,
        max_length=256,
        optim="adamw_torch",
        report_to="none",
    )

    trainer = SFTTrainer(
        model=model,
        train_dataset=train_dataset,
        args=training_args,
        processing_class=tokenizer,
    )

    print("Training...")
    t0 = time.time()
    train_result = trainer.train()
    train_time = time.time() - t0
    print(f"Training complete in {train_time:.1f}s")
    print(f"  Loss: {train_result.training_loss:.4f}")

    # ‚îÄ‚îÄ Step 5: Post-vaccination test ‚îÄ‚îÄ
    print("\n‚îÄ‚îÄ Step 5: Post-vaccination nightmare test ‚îÄ‚îÄ")
    model.eval()
    post_results = test_nightmare_resistance(model, tokenizer, device, "post")

    post_clean_acc = sum(1 for r in post_results if r["clean_correct"]) / len(post_results) * 100
    post_nightmare_acc = sum(1 for r in post_results if r["nightmare_correct"]) / len(post_results) * 100
    post_mean_spike = np.mean([r["entropy_spike"] for r in post_results])

    print(f"  Post-vaccination:")
    print(f"    Clean accuracy:     {post_clean_acc:.1f}%")
    print(f"    Nightmare accuracy: {post_nightmare_acc:.1f}%")
    print(f"    Mean entropy spike: {post_mean_spike:+.4f}")

    # ‚îÄ‚îÄ Analysis ‚îÄ‚îÄ
    nightmare_improvement = post_nightmare_acc - pre_nightmare_acc
    spike_change = post_mean_spike - pre_mean_spike

    print("\n" + "=" * 70)
    print("VACCINATION EFFECTIVENESS")
    print("=" * 70)
    print(f"  Clean accuracy:     {pre_clean_acc:.1f}% ‚Üí {post_clean_acc:.1f}%")
    print(f"  Nightmare accuracy: {pre_nightmare_acc:.1f}% ‚Üí {post_nightmare_acc:.1f}% ({nightmare_improvement:+.1f}%)")
    print(f"  Entropy spike:      {pre_mean_spike:+.4f} ‚Üí {post_mean_spike:+.4f} ({spike_change:+.4f})")

    if nightmare_improvement > 0:
        print(f"\n  ‚úÖ VACCINATION EFFECTIVE! +{nightmare_improvement:.1f}% nightmare resistance")
    elif nightmare_improvement == 0:
        print(f"\n  ‚öñÔ∏è No significant change in nightmare resistance")
    else:
        print(f"\n  ‚ö†Ô∏è Nightmare resistance decreased (may need more data or epochs)")

    # ‚îÄ‚îÄ Save results ‚îÄ‚îÄ
    os.makedirs(RESULTS_DIR, exist_ok=True)
    os.makedirs(FIGURES_DIR, exist_ok=True)

    all_results = {
        "pre_vaccination": {
            "clean_accuracy": round(pre_clean_acc, 1),
            "nightmare_accuracy": round(pre_nightmare_acc, 1),
            "mean_entropy_spike": round(float(pre_mean_spike), 4),
            "results": pre_results,
        },
        "post_vaccination": {
            "clean_accuracy": round(post_clean_acc, 1),
            "nightmare_accuracy": round(post_nightmare_acc, 1),
            "mean_entropy_spike": round(float(post_mean_spike), 4),
            "results": post_results,
        },
        "training": {
            "loss": round(train_result.training_loss, 4),
            "time_seconds": round(train_time, 1),
            "epochs": 1,
            "lora_r": 16,
            "learning_rate": 2e-4,
            "train_samples": len(train_dataset),
        },
        "improvement": {
            "nightmare_accuracy_delta": round(nightmare_improvement, 1),
            "entropy_spike_delta": round(float(spike_change), 4),
        },
    }

    results_path = os.path.join(RESULTS_DIR, "phase4_results.json")
    with open(results_path, "w", encoding="utf-8") as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False, cls=NumpyEncoder)
    print(f"\nüíæ Results: {results_path}")

    # ‚îÄ‚îÄ Visualization ‚îÄ‚îÄ
    fig, axes = plt.subplots(1, 3, figsize=(16, 5))
    fig.suptitle("Project Genesis ‚Äî Phase 4: QLoRA Vaccination\n"
                 f"SNN-Generated Data ({len(train_dataset)} samples, 1 epoch)",
                 fontsize=13, fontweight="bold")

    # Panel 1: Accuracy comparison
    ax = axes[0]
    x = np.arange(2)
    width = 0.3
    pre_vals = [pre_clean_acc, pre_nightmare_acc]
    post_vals = [post_clean_acc, post_nightmare_acc]
    ax.bar(x - width/2, pre_vals, width, label="Pre-vaccination", color="#e74c3c", alpha=0.7)
    ax.bar(x + width/2, post_vals, width, label="Post-vaccination", color="#2ecc71", alpha=0.7)
    ax.set_xticks(x)
    ax.set_xticklabels(["Clean", "Nightmare"])
    ax.set_ylabel("Accuracy (%)")
    ax.set_title("Accuracy: Before vs After Vaccination")
    ax.legend()
    ax.set_ylim(0, 105)
    for i, (pre, post) in enumerate(zip(pre_vals, post_vals)):
        ax.text(i - width/2, pre + 1, f"{pre:.0f}%", ha="center", fontsize=9)
        ax.text(i + width/2, post + 1, f"{post:.0f}%", ha="center", fontsize=9)

    # Panel 2: Per-question entropy spike comparison
    ax = axes[1]
    q_labels = [f"Q{i+1}" for i in range(len(pre_results))]
    pre_spikes = [r["entropy_spike"] for r in pre_results]
    post_spikes = [r["entropy_spike"] for r in post_results]
    x = np.arange(len(q_labels))
    ax.bar(x - width/2, pre_spikes, width, label="Pre", color="#e74c3c", alpha=0.7)
    ax.bar(x + width/2, post_spikes, width, label="Post", color="#2ecc71", alpha=0.7)
    ax.set_xticks(x)
    ax.set_xticklabels(q_labels, fontsize=7)
    ax.set_ylabel("Entropy Spike (ŒîH)")
    ax.set_title("Canary Alarm per Question")
    ax.legend()
    ax.grid(alpha=0.3, axis="y")

    # Panel 3: Summary
    ax = axes[2]
    ax.axis("off")
    summary = (
        f"Genesis Vaccination Summary\n"
        f"{'=' * 40}\n\n"
        f"Training:\n"
        f"  Samples:  {len(train_dataset)}\n"
        f"  Epochs:   1\n"
        f"  LoRA r:   16\n"
        f"  Loss:     {train_result.training_loss:.4f}\n"
        f"  Time:     {train_time:.0f}s\n\n"
        f"Results:\n"
        f"  Clean acc:     {pre_clean_acc:.0f}% ‚Üí {post_clean_acc:.0f}%\n"
        f"  Nightmare acc: {pre_nightmare_acc:.0f}% ‚Üí {post_nightmare_acc:.0f}%\n"
        f"  Œî nightmare:   {nightmare_improvement:+.1f}%\n"
        f"  Œî entropy:     {spike_change:+.4f}\n\n"
    )
    if nightmare_improvement > 0:
        summary += f"  ‚úÖ VACCINATION EFFECTIVE!"
    elif nightmare_improvement == 0:
        summary += f"  ‚öñÔ∏è No significant change"
    else:
        summary += f"  ‚ö†Ô∏è Need more data/epochs"

    ax.text(0.05, 0.95, summary, transform=ax.transAxes,
            fontsize=10, verticalalignment="top", fontfamily="monospace",
            bbox=dict(boxstyle="round", facecolor="lightyellow", alpha=0.5))

    plt.tight_layout()
    plot_path = os.path.join(FIGURES_DIR, "phase4_vaccination.png")
    plt.savefig(plot_path, dpi=150, bbox_inches="tight")
    plt.close()
    print(f"üìä Chart: {plot_path}")

    # Cleanup
    del model, tokenizer, trainer
    torch.cuda.empty_cache()

    print("\n" + "=" * 70)
    print("Phase 4 Complete!")
    print(f"  ‚Üí Nightmare improvement: {nightmare_improvement:+.1f}%")
    print(f"  ‚Üí Next: Phase 5 ‚Äî Evolution Loop (iterate vaccine generation)")
    print("=" * 70)


if __name__ == "__main__":
    run_phase4()
